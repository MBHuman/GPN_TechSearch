{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13,\n",
       " 14,\n",
       " 4,\n",
       " 12,\n",
       " 9,\n",
       " 27,\n",
       " 5,\n",
       " 6,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 11,\n",
       " 10,\n",
       " 28,\n",
       " 21,\n",
       " 18,\n",
       " 8,\n",
       " 20,\n",
       " 22,\n",
       " 19,\n",
       " 23,\n",
       " 25,\n",
       " 26,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 24]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #,CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import logistic\n",
    "from scipy.spatial.distance import cosine as dist\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "input_data = pd.read_csv('input.csv')\n",
    "\n",
    "input_data = input_data[['name','description','category']]\n",
    "\n",
    "input_data['description'] = input_data['description'].apply(lambda x: x.split(',')[-2])\n",
    "input_data['city'] = input_data['description']\n",
    "input_data = input_data.drop('description', axis=1)\n",
    "\n",
    "\n",
    "input_data = input_data.rename(columns={'name':'name_company', \n",
    "                             'category':'categories',\n",
    "                             'description':'city'})\n",
    "\n",
    "\n",
    "#####################\n",
    "## Структура данных ##\n",
    "#####################\n",
    "\n",
    "####################################################################################\n",
    "####################################################################################\n",
    "'''\n",
    "data_structure = {'name_company': object, \n",
    "                  'categories':object,\n",
    "                  'city':object\n",
    "                 } \n",
    "           \n",
    "Требования к данным:\n",
    "1. 'name_company' - название компании\n",
    "    - может быть и пустой строкой вида '' (заполнить)\n",
    "    - имеет ограничение по количеству символов ~30\n",
    "2. 'categories' - отрасль компании\n",
    "    - НЕ МОЖЕТ быть пустой строкой (иначе компания вообще не будет индексироваться и попадать в выдачу)\n",
    "    - имеет ограничение по количеству символов ~200   \n",
    "3. 'city' - город в котором зарегистрирована компания\n",
    "    - имеет ограничение по количеству символов ~30\n",
    "    - НЕ МОЖЕТ быть пустой строкой\n",
    "\n",
    "           \n",
    "'''\n",
    "\n",
    "data_structure = {'name_company': object, \n",
    "                  'categories':object,\n",
    "                  'city':object\n",
    "                 } \n",
    "\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "###########################\n",
    "##  Здесь проверки качества ##\n",
    "##########################\n",
    "\n",
    "def checking_data(data):\n",
    "    for col in data.columns:\n",
    "        try:\n",
    "            if data_structure[col] in [int, float]:\n",
    "                fill = 0\n",
    "            elif data_structure[col] == object:\n",
    "                fill = ''\n",
    "            data[col] = data[col].fillna(fill)\n",
    "            try:\n",
    "                if isinstance(data.loc[0, col], data_structure[col]) == False:\n",
    "                    data[col] = data[col].astype(data_structure[col])\n",
    "            except:\n",
    "                print('Неверный выходной тип данных. Колонка {}. Ожидалось {}. Пришло {}'.format(col, \n",
    "                                                                                               data_structure[col],\n",
    "                                                                                               type(data.loc[0, col])))\n",
    "        except:\n",
    "            print('Проблема с типом входных данных. Неизвестный формат у {}'.format(col))\n",
    "    return data\n",
    "\n",
    "data = checking_data(input_data)\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "comp_names = data['name_company'].tolist()\n",
    "documents = data['categories'].tolist()\n",
    "city = data['city'].tolist()\n",
    "\n",
    "TARGET = \"металлопрокат, цветные металлы самара \"\n",
    "\n",
    "data['categories'] = documents\n",
    "\n",
    "stoplist = set(nltk_stopwords.words('russian'))\n",
    "\n",
    "def clean_text(expression, replacement, text):\n",
    "    text_wo_re = re.sub(expression, replacement, text)\n",
    "    return ' '.join(text_wo_re.split())\n",
    "\n",
    "def lemmatize(text, expression):\n",
    "    m = Mystem()\n",
    "    clear_text = clean_text(expression, ' ', text)\n",
    "    lemm_text_list = m.lemmatize(clear_text)\n",
    "    return clean_text(expression, ' ', \" \".join(lemm_text_list))\n",
    "\n",
    "regexp=r'[^а-яА-ЯeЁ0-9]'\n",
    "\n",
    "def lemm_list(text, expression):\n",
    "    lemm = []\n",
    "    for text in data['categories']:\n",
    "        lemm.append(lemmatize(text, expression=regexp))\n",
    "    return lemm\n",
    "\n",
    "lemm_categories = lemm_list(data['categories'], regexp)    \n",
    "    \n",
    "TARGET = lemmatize(TARGET, regexp)\n",
    "\n",
    "def stemmer(sentense):\n",
    "    words = sentense.split(' ')\n",
    "    snow_stemmer = SnowballStemmer(language='russian')\n",
    "    stemm_sentense = []\n",
    "    for word in words:\n",
    "        stemm_sentense.append(snow_stemmer.stem(word))\n",
    "    return ' '.join(stemm_sentense)\n",
    "\n",
    "stemm_target = stemmer(TARGET)\n",
    "\n",
    "def is_query_in_title(comp_names, TARGET):\n",
    "    companies = []\n",
    "    for i in comp_names:\n",
    "        companies.append(i.lower())\n",
    "    num_inter = {}\n",
    "    counter = 0\n",
    "    for i in comp_names:\n",
    "        num_inter[str(counter)] = 0\n",
    "        for j in TARGET.split():\n",
    "            if j in i.lower():\n",
    "                num_inter[str(counter)] += 1\n",
    "        num_inter[str(counter)] = num_inter[str(counter)] / (len(i.split()) + len(stemm_target.split()))\n",
    "        counter += 1\n",
    "    return num_inter\n",
    "\n",
    "\n",
    "def is_city_in_query(cities_list, TARGET):\n",
    "    TARGET = TARGET.split()\n",
    "    cities = []\n",
    "    for i in cities_list:\n",
    "        i = i.strip().lower()\n",
    "        if i in TARGET:\n",
    "            cities.append(1)\n",
    "        else: \n",
    "            cities.append(0)\n",
    "    return cities\n",
    "\n",
    "\n",
    "def rait_categories(lemm_categories, TARGET):\n",
    "    \n",
    "    global stoplist, regexp\n",
    "    \n",
    "    texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in lemm_categories\n",
    "    ]\n",
    "    \n",
    "    # remove words that appear only once\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    lemm_categories = [\n",
    "        [token for token in text if frequency[token] >= 1]\n",
    "        for text in texts\n",
    "    ]\n",
    "\n",
    "    dictionary = corpora.Dictionary(lemm_categories)\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemm_categories]\n",
    "\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "\n",
    "    vec_bow = dictionary.doc2bow(TARGET.lower().split())\n",
    "    vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    sims = index[vec_lsi]\n",
    "\n",
    "    ### не уверен в работе этого ###\n",
    "    index.save('/tmp/deerwester.index')\n",
    "    index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')\n",
    "\n",
    "    sims = enumerate(sims)\n",
    "    doc_rait = []\n",
    "    doc_pos = []\n",
    "    for doc_position, doc_score in sims:\n",
    "        doc_rait.append(doc_score)\n",
    "        doc_pos.append(doc_position)\n",
    "        \n",
    "    data = pd.Series(doc_rait)\n",
    "    return data\n",
    "\n",
    "\n",
    "def rait_by_distance(lemm_categories, TARGET):\n",
    "    global stoplist, regexp\n",
    "    # Инициализируем векторайзер\n",
    "    count_tf_idf = TfidfVectorizer(stop_words=stoplist)\n",
    "    # Векторизуем корпус текстов \"категории\"\n",
    "    \n",
    "    tf_idf_key = count_tf_idf.fit_transform(lemm_categories)    \n",
    "        \n",
    "     # Векторизуем таргет\n",
    "    tf_idf_target = count_tf_idf.transform([TARGET])\n",
    "\n",
    "    # Создаем новый датафрейм, куда помещаем векторизованные сущности\n",
    "    text = pd.DataFrame()\n",
    "    text['vect_keys'] = list(tf_idf_key.toarray())\n",
    "\n",
    "    # Костыль для заполнения столбца таргета вектором\n",
    "    target_list = list(tf_idf_target.toarray())\n",
    "    text['target'] = 0\n",
    "    text['target'] = text['target'].apply(lambda x: target_list )\n",
    "\n",
    "    # Теперь вычисляем косинусное расстояние между текстами\n",
    "    text['dist_keys']=text.apply(lambda x: dist(x['vect_keys'],x['target']),axis=1)\n",
    "\n",
    "    # Переводим косинусное расстояние в меру близости\n",
    "    text['proxi_keys'] = 1 - text['dist_keys'] \n",
    "    text = text.fillna(0)\n",
    "    return text[['proxi_keys']] #text[['proxi_desc', 'proxi_keys']]\n",
    "\n",
    "dist_data = rait_by_distance(lemm_categories, TARGET)\n",
    "\n",
    "relevance_data = pd.DataFrame(data={'rait_cat':rait_categories(lemm_categories, TARGET),\n",
    "                                    #'proxi_desc':dist_data['proxi_desc'],\n",
    "                                    'proxi_keys':dist_data['proxi_keys']\n",
    "})\n",
    "\n",
    "\n",
    "relevance_data['word_in_name'] = is_query_in_title(comp_names, stemm_target).values()\n",
    "relevance_data['cities_matched'] = is_city_in_query(city, TARGET)\n",
    "\n",
    "#Вычисляем итоговый рейтинг компаний\n",
    "'''\n",
    "relevance_data['final_reit'] = (relevance_data.rait_cat * relevance_data.proxi_keys * ( 1 + (\n",
    "                             relevance_data.comm_weight + relevance_data.rait + relevance_data.proxi_desc +\\\n",
    "                             relevance_data.have_email))) / (1 - relevance_data.word_in_name)\n",
    "'''\n",
    "\n",
    "relevance_data['final_reit'] = (relevance_data.rait_cat * relevance_data.proxi_keys / (1 - relevance_data.word_in_name))\n",
    "\n",
    "\n",
    "sorted_relevance_data = relevance_data.join(input_data['categories']).sort_values(['cities_matched', 'final_reit'], \n",
    "                                                                ascending=[False, False])\n",
    "#display(sorted_relevance_data)\n",
    "\n",
    "#if sorted_relevance_data.loc[sorted_relevance_data['cities_matched'] == 1]['cities_matched'].count() == 0:\n",
    "#    print('Извините, по указанному региона поиска результатов выдачи нет. Но вы можете посмотреть на компании из других регионов:')\n",
    "final_output = sorted_relevance_data.index.to_list()\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
